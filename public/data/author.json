{
  "author": {
    "id": "1",
    "name": "Beth Simone Noveck",
    "profilePicture": "https://innovation.nj.gov/assets/images/team/beth.png",
    "shortDescription": "Director @TheGovLab, Prof @Northeastern Former @POTUS44 US Deputy CTO White House @OpenGov Initiative. Author, Solving Public Problems (2021), Smart Citizens, Smarter State (2015), Wiki Government (2009) and State of Play (2005)",
    "aboutMe": "<p>Beth Simone Noveck is a professor at Northeastern University, where she directs the Burnes Center for Social Change and its partner project, The Governance Lab (The GovLab). She is faculty at the Institute for Experiential AI, School of Law, and in the College of Social Sciences and Humanities, the College of Arts, Design, and Media, the College of Engineering, and affiliated faculty at the Khoury College of Computer Sciences.</p>\n<p>Beth’s work focuses on using AI to reimagine participatory democracy and strengthen governance, and she has spent her career helping institutions incorporate more equitable and open ways of working using new technology. Her newest book Democracy Rebooted: How AI Can Save Democracy will appear with Yale University Press,</p>\n<p>Among her many civic technology projects, she created Unchat, one of the first online platforms for democratic deliberation, Peer-to-Patent to connect scientists to policymakers to improve the patent process, The Cairns Project for citizen co-creation, and two decades before the Metaverse, she built Democracy Island in Second Life to test out new ways for people to engage with one another civically online.</p>\n<p>Today, she leads the GovLab's InnovateUS initiative, which trains public sector professionals in AI, digital and innovation skills. </p>\n<p>Beth has been working with students to build technology for social good. She launched Northeastern's new AI for Impact Co-Op, building on twenty years of experience teaching experiential civic technology clinics. Beth is the founder of open, online courses such as Solving Public Problems, which has helped social innovators in over 100 countries take a project from idea to implementation and Open Justice for legal innovators.</p>\n<p>Previously, Beth served in the White House as the first United States Deputy Chief Technology Officer under President Obama. She founded the White House Open Government Initiative, which created policies and platforms, such as data.gov and challenge.gov, for making the federal government more transparent, participatory, and collaborative. </p>\n<p>She served as senior advisor for Open Government for UK Prime Minister David Cameron, and between 2018-2021, she served on Chancellor Angela Merkel's Digital Council.</p>\n<p>Appointed the State of New Jersey’s first Chief AI Strategist by Governor Phil Murphy, she previously served as the state's Chief Innovation Officer (2018-2024). The Office of Innovation uses new technology to improve equitable  delivery of government services. The Office has worked with partner agencies to modernize unemployment insurance, provide a whole-of-government response to COVID, collect real-time infection data, deliver everything needed to start, run and grow a business, and use open data to provide training information to job seekers and improve uptake of benefits, services and permits. She also serves as Co-Chair of the State's AI Taskforce and previous led the State's Future of Work Task Force</p>\n<p>She is also the author of three earlier books, Solving Public Problems: How to Fix Our Government and Change Our World (Yale Press 2021) was named a Best Book of the year by Stanford Social Innovation Review.</p>\n<p>She was named one of the “Foreign Policy 100” by Foreign Policy, one of Fast Company’s “100 Most Creative People in Business”, a “Top Women in Technology” by Huffington Post, and one of the World's 100 Most Influential Academics in Government by Apolitical. She was awarded a doctorat honoris causa by the University of Geneva. Her TED talk is here, her publications can be found below and she blogs at RebootDemocracy.ai. </p>",
    "posts": [
      {
        "id": "101",
        "title": "Was vTaiwan such a big flop, after all?",
        "picture": "https://content.thegovlab.com/assets/829a327b-dd3f-4f56-80d5-dc3249755aaa?width=800",
        "body": "<p>After vTaiwan enabled over two hundred thousand people to participate in crafting 26 pieces of national legislation, advocates for tech and democracy hailed this four-stage online and offline deliberative process as the poster child of tech-enabled public engagement. We celebrated vTaiwan as evidence of the powerful potential for meaningful public participation in governance.</p><p>vTaiwan began with a proposal stage, with offline and online discussion of problems using a series of different tools for deliberation and frequent polling. This collaborative problem-definition process, which lasted from a few weeks to a year, helped a large number of people to agree on and define which problems should be tackled.</p><p>While disappointing, vTaiwan is not unique in failing to deliver on the promise of tech-enabled participation. As my GovLab colleagues and I reported last year, Madrid’s online engagement platform Decide Madrid attracted almost half a million sign-ups. But of the 28,000 legislative proposals submitted by residents since 2015, only one became policy. Sign-ups have declined dramatically.</p><p>Online public engagements fizzle for a variety of reasons. First, they can be hard for the public to use. With four different platforms cobbled together, Hsu reports that vTaiwan was seen as difficult to use and people lost interest in navigating the complex process. Indeed, the project relied on a number of different tools, none of which had a particularly intuitive user interface.</p><p>However, even when the public participation platforms are easy for the public to use, more often than not they are poorly designed for the sponsoring institutions, reducing participation to simplistic contributions that only serve to drown institutions in voluminous, mostly unusable feedback. That’s what happened in Madrid where short comments without supporting evidence or blue prints for implementation lacked the necessary complexity to translate an idea into action. It’s not clear how well vTaiwan meshed with the way lawmaking worked.</p><p>Poor design, however, might only be partly to blame. The success of the project might also have threatened the political status quo and presented a challenge to traditional forms of decision making and political power. We wrote this report focusing on how to institutionalize public engagement in decision making, attempting to draw lessons across a wide range of contexts and geographies. vTaiwan, more than many online engagement projects, was designed to fit with the legislative process.</p><p>One of the critical factors we identified was political support. While Audrey Tang, Taiwan’s Digital Minister, was co-creator and driver behind the project, it’s not clear from afar how much support the project enjoyed outside or inside of government.</p><p>As we explore new uses of artificial intelligence to make resident engagement better suited to complex policy making processes, we will need to look more deeply at what went wrong in Taiwan and whether it might have been vTaiwan’s success, rather than (or in addition to any failures of design) that killed the project.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-06-01",
        "views": 1500,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "102",
        "title": "Sam Altman, One True Leader and the Missed Opportunity for Innovation",
        "picture": "https://content.thegovlab.com/assets/202ef7d2-9315-4019-b9ea-2e2e6480892b?width=800",
        "body": "<p>In his forthcoming and beautifully-written review in Constitutional Commentary of Steven Levitsky and Daniel Ziblatt's new book Tyranny of the Minority, San Diego Law School Professor Laurence Claus asks why the authors' prescription for democratic reform neglects to address what he calls the \"one true leader.\" The One True Leader refers to the office of the chief executive.</p><p>\"Many people can become in thrall to a person,\" Claus writes. \"No one has ever been in thrall to a committee. If we don’t want one person at the top for life, then how about not having one person at the top at all?\" </p><p>I live in a town that does not have a Mayor. My community is governed by a five-member Select Board and an elected town meeting and yet the trash still gets taken out. Switzerland has long had a federal council and lived in peace and prosperity.</p><p>It's such a simple question and yet one that rarely gets asked. If the office of the chief executive creates all the risk of democratic subversion, why do we content ourselves by tinkering with an anemic and fragile oversight system instead of fixing the root cause of the problem? If we can have multi-member courts and legislatures, why not a plural executive? Claus rightly challenges us to answer why we do so little to reinvent the role of the charismatic leader.</p><p>To be sure, simply replacing the one with the few, especially where there is a risk of one-party dominance, does not solve all the problems.</p><p>As we saw with the unfolding drama at OpenAI this week, the unitary leader holds a great deal of appeal. Perhaps had the board been more forthcoming, transparent and democratic in how it functioned, the story might have had a different ending. There was a failed opportunity to reimagine governance using the very tools that OpenAI creates.</p><p>Why not use AI to engage more people in decision making?</p><p>Why not take advantage of the tools' efficiencies to foster a more open conversation?</p><p>Why not at least announce a period of reflection and listening to invent and reimagine alternatives?</p><p>It's disappointing, to say the least, that OpenAI's only experiments with democracy thus far involve replicating toothless deliberative assemblies that have no power instead of exploring meaningful ways of engaging the broader public and diverse experts to improve decision making. So much money going into R&D and yet nothing being spent on the democratic innovations that AI could enable.</p><p>We are overdue for a conversation about \"the one true leader\" in the boardroom and in the White House. Alternatives have long been dismissed as inefficient and ineffectual. But the assumptions that undergirded those conclusions are hardly true any longer in the age of AI.</p><p>As we navigate the complexities of leadership in the age of AI and beyond, the question of how we structure our systems of governance – both in politics and in our organizations – remains crucial. The allure of the one true leader may be a familiar path, but it's one fraught with risks of authoritarianism and inefficiency in a rapidly evolving world. The examples of collective leadership, from local town councils to nations like Switzerland, offer a glimpse into the potential of a more distributed, democratic approach.</p><p>The future may depend on our willingness to rethink the very notion of power and authority, leveraging the tools and insights of our time to forge more resilient, inclusive, and effective governance structures.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-06-15",
        "views": 2500,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "103",
        "title": "The AI Echo Chamber: New Paper on AI and Epistemic Risk",
        "picture": "https://content.thegovlab.com/assets/54d58fb7-e9d1-4165-b5d5-654bee33a479?width=800",
        "body": "<p>In his paper AI and Epistemic Risk for Democracy: A Coming Crisis of Public Knowledge, Northeastern professor John P. Wihbey argues that AI poses significant epistemic risks to democratic societies by mediating and shaping the informational domains that support public knowledge and deliberation.</p><p>Wihbey pinpoints three major questions confronting scholars of AI and democracy: AI alignment, mechanism design, and the new issue he focuses on: epistemic risk.</p><p>The AI alignment problem refers to the challenge of ensuring that AI systems operate in accordance with human values and preferences, even as they evolve over time. Mechanism design, on the other hand, pertains to the challenge of structuring technological platforms and AI models in a way that allows humans to express their genuine views and have them accurately reflected in democratic decision-making processes. While these two challenges have been the primary focus of the AI ethics and governance discourse, Wihbey argues that epistemic risk deserves equal attention.</p><p>The question of what constitutes a healthy informational diet for democracy has long preoccupied scholars and policymakers, particularly in the study and regulation of broadcast journalism. Much of the European speech tradition is predicated upon the notion that democracy rests upon a society of well-informed citizens having access to a wide range of information, having the freedom to form opinions based on the available information and to express those opinions. The informational diet of democracy must be varied and colorful!</p><p>However, Wihbey contends that the rise of AI presents new challenges to the democratic knowledge ecosystem.</p><p>He worries about automated journalism. Imagine an AI system that writes news articles based only on past data. It might keep using the same outdated storylines and facts, missing out on new information, changing opinions, and fresh perspectives that are important to people today.</p><p>He also uses the example of social media moderation. AI chatbots might accidentally delete or limit genuine human discussions, because the chatbots' decisions are based on old data that doesn't keep up with the fast-paced, always-changing nature of online talking points and trends.</p><p>Suppose opinion polls start using AI-generated survey responses instead of real people's answers. The AI might give incorrect predictions about what the public thinks on current topics, because it can't properly account for shifts in beliefs or changes in population groups over time. These skewed poll results could then wrongly influence how people really think and act.</p><p>As search engines use AI to condense information into short, simplified snippets, people may start relying too heavily on these AI-generated summaries. This could lead to people spending less time exploring, discovering, and thinking critically for themselves, and instead just accepting the AI's limited and potentially biased take on knowledge.</p><p>AI systems, trained on data from the human past, may struggle to capture the emergence of new knowledge, values, and preferences that arise through dynamic human interaction. This could lead to a recursive feedback loop, where AI's representation of reality shapes public perception and choice, which in turn reinforces the AI's limited epistemological framework.</p><p>The \"epistemic risk\" perspective he offers challenges us to think beyond the immediate benefits and drawbacks of AI in specific domains and to consider the broader, systemic impact of AI on the health and resilience of our democratic knowledge ecosystem as a whole. It raises important questions about the long-term compatibility of AI-mediated information ecosystems with the principles of democratic deliberation and collective self-determination.</p><p>I do wonder, however, whether the risk is greater than the limitations of the current human-driven information ecosystem. Today, public knowledge is heavily mediated by the subjective judgments, biases, and agendas of human gatekeepers, such as news editors and content moderators. By contrast, AI-powered systems also have the potential to increase content diversity and surface underrepresented viewpoints by drawing upon a vast range of data sources and employing algorithms designed to prioritize balance and inclusivity. AI could help counter issues like echo chambers and political polarization by exposing individuals to a broader spectrum of ideas and information.</p><p>Moreover, as he points out, concerns about AI's epistemological limitations may be mitigated through advances in machine learning techniques, such as reinforcement learning and transfer learning, which could enable AI systems to adapt more dynamically to evolving social realities. As AI becomes more sophisticated in its ability to process and generate human-like content, it may grow more responsive to the emergence of new knowledge and values.</p><p>Nevertheless, Wihbey's argument serves as a valuable reminder and caution that the development of AI systems for knowledge production and dissemination must be approached with attention to the question of the informational diet of democracy. If AI comes to dominate key informational domains without adequate safeguards and human oversight, it could inadvertently ossify public discourse and constrain the organic evolution of human understanding.</p><p>Ultimately, the path forward lies first in recognizing the information quality problems and developing hybrid human-AI systems that harness the benefits of algorithmic content generation and curation while preserving space for the serendipitous, open-ended evolution of human insight and discourse. This will require ongoing research, experimentation, and public dialogue to strike a balance between the transformative potential of AI and the safeguarding of democratic epistemic health undergirded by a steadfast commitment to the principles of open inquiry, pluralism, and collective self-determination.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-05-25",
        "views": 1000,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "104",
        "title": "Digital Mirror to Our Deliberation",
        "picture": "https://content.thegovlab.com/assets/424795a9-1428-49f2-8155-0c0c42fc6b4d?width=800",
        "body": "<p>Last night at a restaurant, I overheard a conversation. A man was discussing how the junior female engineers on his team often end up taking notes during company brainstorming sessions. His female companion suggested assigning this task among male team members. But the conversation highlighted a missed opportunity for using AI transcription tools, such as Otter.ai, Wudpecker or Fathom, which can revolutionize meeting dynamics by eliminating the need for manual note-taking altogether, overcoming hurtful gender dynamics and fostering a more inclusive and productive environment.</p><p>Despite the fact that we spend so much of our time in meetings, we know too little about how to run them well, a point Sir Geoff Mulgan has made thoughtfully in his excellent book on collective intelligence Big Mind. AI transcription tools offer far-reaching potential to transform meetings and professional practices across an array of industries. In a recent training I did for government lawyers, I could feel the room collectively “perk up” when I demonstrated AI transcription. The ability to upload an audio file of a deposition, phone call, or hearing and have the tool transcribe, summarize and extract learnings from a lengthy audio recording could potentially save them untold hours of time. My students, too, like to record their classes, upload the file, and use these tools to help them study.</p><p>Parliaments around the world are turning to AI transcription to create real-time records of floor debate. In India, the parliament’s Digital Sansad software takes advantage of AI to provide members and the public with real-time transcription but also translation, capturing word-for-word what is said in parliament and translating into one of India's twenty-two regional languages and dialects.</p><p>However, these tools are more than just a faster stenographer. I love the fact that AI transcription shines a light on conversational dynamics by calculating how much time each person speaks during a meeting, paving the way for improved social relations, better behavior and more equitable conversations.</p><p>The ability for AI transcription to lay bare how we talk to one another can help us to democratize formal as well as informal conversational interactions. Law professor Tonja Jacobi and her colleagues studied patterns of interruptions at the Supreme Court by reviewing hearing transcripts, revealing how often male justices interrupted female ones: a lot.</p><p>Their research led to a rule change in 2021, whereby justices now ask questions in order of seniority. (As a by-product of the research, now Justice Clarence Thomas participates more often.)The law professors had to hand code their data.</p><p>But AI now makes light work of such conversational pattern analysis, revealing biases and bad behavior and, when used well, creating incentives for better communications. The self-reflection enabled by AI has the potential to revolutionize all forms of group deliberation from parliaments to town halls to boardrooms with AI serving as a mirror to our conversations and group interactions.</p><p>The adoption of AI transcription technologies represents more than just an upgrade to our professional toolkits; it could offer a pivotal shift towards inclusivity and equity in every conversation we have. By democratizing participation in meetings and ensuring every voice is heard—and accurately captured—these technologies could help us to overcome entrenched biases and foster richer, more productive dialogues. This is especially important in deliberative dialogues, whether at the national or local level, where we need diverse people and perspectives to have an equal opportunity of being heard. Leaders, educators, and policymakers need to explore the integration of AI transcription tools not only to enhance efficiency but to pave the way for deliberations where all contributions are valued.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-07-10",
        "views": 3000,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "105",
        "title": "Guns, Narratives, and AI",
        "picture": "https://content.thegovlab.com/assets/64af4aa9-f321-4c2a-aefb-f751392912d3?width=800",
        "body": "<p>CNN recently reported (and we showcased in this blog’s News of the Week) that parents of some victims of the Parkland shooting used AI to recreate their children's voices, generating over 54,000 AI-voiced calls to lawmakers to urge Congress to address gun violence. The fight for gun control underscores the urgent need to shift the narrative around AI, if we are to harness its benefits effectively.</p><p>In his new book, What We’ve Become, Dr. Jonathan Metzl, my colleague at the Burnes Center for Social Change, posits that the battle for gun control is hindered by a public health narrative that naively overlooks liberty concerns. The freedom to protect oneself, often cited as a defense against a perceived overreaching state (and from a non-white, immigrant “other”) frequently overrides concerns about injury or death. This liberty—or really fear—narrative helps to explain why, in the wake of the horrific 2018 Tennessee Waffle House mass killings that are the centerpiece of Metzl’s excellent book, residents elected (and later re-elected) a candidate who successfully pushed through permitless carry. Metzl’s bottom line message? If we want to reduce gun violence, advocates need to take on the liberty argument.</p><p>The same government-is-the-problem-not-the-solution narrative has direct parallels to the conversation unfolding about artificial intelligence. Titans of tech have peddled the view that they (not tech-illiterate politicians in Washington) know better how to run the world. Adrienne LaFrance in The Atlantic critiques this as an \"antidemocratic, illiberal movement,\" while Matteo Wong highlights the doomerism promoted by tech leaders.</p><p>It’s “boom time for doom time,” as Bryan Walsh says in a recent Vox article. But this simplistic, fear-induced narrative needs to be countered by a more nuanced and complex conversation about how to use AI to help us solve big problems.</p><p>The opposite of doomerism isn’t an equally naive utopianism. Instead, what we need in our approach to policy, research and journalism is a more sober and balanced discussion about ways to advance the public interest here and now and the hurdles that impede those advances.</p><p>We could have a good laugh over the breathless hyperbole coming out of Silicon Valley (and parroted in Washington) were this narrative not infecting our policymaking, dominating media headlines, and driving the focus of academic research. As with concerns about the liberty narrative in the gun debate, we need more conscious awareness of the fact that the doomerist narrative is manufactured to reinforce a distrust in government.</p><p>Doomerism distorts our perception of the important role the government needs to play, first, to safeguard civil rights but also to advance the use of technology for the public good through practical investment and proactive policies that address how to responsibly use these technologies and ensure they work for the public interest.</p><p>Speculative, alarmist outlooks are skewing the discourse away from how to do more today to advance medical breakthroughs, rescue endangered languages, fight climate change and improve the workings of government using AI. Such efforts are overshadowed in the headlines by Chicken Little-style narratives that the sky is falling. AI ethics need to go beyond focusing predominantly on risk avoidance and address how to proactively promote public interest uses of technology that advance equity and deepen democracy.</p><p>As we have seen with guns, the narrative matters. As Metzl commented in Time Magazine: \"Democrats need to tie gun safety to the defense of the American public square.\" When we are told that we need to safeguard our liberty against an encroaching state, we go out to buy a gun despite the risks to life and limb. When we are wrongly (or at least exaggeratedly) told that artificial general intelligence is around the corner, we invest time, money and attention in worrying about the robot apocalypse, instead of asking how we can use these data processing tools despite their many flaws to reduce inequality, combat discrimination, and create a better world. Yet, if we are to counter this narrative, we both have to recognize and engage with it. We have to address the fears that Silicon Valley, the media, academics, and politicians have fomented and then go beyond it, addressing how AI can also respond to risks, not simply create them.</p><p>By adopting a balanced, evidence-based approach to discussing AI, we can foster a more productive dialogue that prioritizes current benefits over speculative dangers, paving the way for AI to contribute meaningfully to societal advancement.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-07-01",
        "views": 1800,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "106",
        "title": "Research Radar: Breaking Language Barriers: EU's iDEM Project Harnesses AI to Empower Marginalized Voices in Democracy",
        "picture": "https://content.thegovlab.com/assets/b20c8bcd-176e-4ce3-8911-88500a206be3?width=800",
        "body": "<p>Authors: Saggion, O’Flaherty, Blanchet, Sharoff, Sanfilippo, Muñoz, Gollegger, Rascón, Martí, Szasz, Bott, Sayman</p><p>Source: SEPLN-CEDI2024: Seminar of the Spanish Society for Natural Language Processing at the 7th Spanish Conference on Informatics</p><p>Question: A newly Horizon-funded EU research initiative, IDEM (Innovative and Inclusive Democratic Spaces for Deliberation and Participation) is studying how deliberative and participatory democratic practices can be made more inclusive and accessible, particularly for marginalized and vulnerable communities with limited language skills?</p><p>Significance: Millions of people in the EU and globally struggle with language barriers (low levels of literacy, intellectual disabilities, dyslexia, aphasia, temporary impairments, elderly or those affected by a significant cognitive, physical, or sensory decline) that restrict their full participation in democracy. The iDEM project seeks to break these linguistic barriers and create a new era of inclusive and participatory democratic spaces for marginalized communities. By promoting inclusivity and representation, iDEM aims to strengthen the legitimacy of deliberative and participatory processes.</p><p>Method: The iDEM project will adopt a user-centered approach to investigate the linguistic barriers hampering participation in deliberative processes. It will create parallel domain-specific annotated datasets for text simplification in Catalan, Spanish, and Italian. Natural language processing models will be fine-tuned to identify and classify sources of text complexity and simplify them accordingly. Tools for generating coherent discourses using fine-tuned Large Language Models will also be developed.</p><p>Experiment: The iDEM project will develop and implement three use cases, each focusing on a different language (Catalan, Italian, or Spanish), intersectionality (people with disabilities, migrants, the elderly), and deliberative approach (citizen's assembly, mini-public, consultation). The use cases will follow a detailed prototyping process to collect user and institutional requirements, develop text simplification functionalities, and create tools for engaging hard-to-reach and vulnerable populations. Facilitators of the deliberative and participatory processes will be interviewed to understand the challenges faced by marginalized groups in political participation.</p><p>Findings: The iDEM project has just launched, and the findings will be based on the evaluation of the use cases by external experts. The evaluation will assess the inclusiveness and accessibility of the proposed solution. The project aims to create next-generation multilingual models that automatically adapt texts to the needs of the people and provide AI tools for unbiased communication.</p><p>Reflection for Democracy: The iDEM project will address how large language models can promote greater inclusivity in participatory processes. It draws attention to the positive potential for using AI to promote inclusion. The study has the potential to create novel participatory spaces that use text simplification technology and natural language generation. By facilitating the participation of marginalized communities in democratic processes, iDEM may contribute to a more inclusive and representative democracy. The project's success could pave the way for a new era of deliberative and participatory practices that prioritize the voices and needs of all citizens, regardless of their language skills or background overcoming outdated conceptions of citizen competence.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-02-03",
        "views": 1990,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "107",
        "title": "NEW BOOK: Why AI Undermines Democracy and What to Do about It",
        "picture": "https://content.thegovlab.com/assets/f9ebe8ce-684e-483f-a1c1-3edc51b5ac6e?width=800",
        "body": "<p>\"Data science can have bad effects on individuals and society - even potentially supporting mass murder and systemic genocide - without necessarily involving evil data scientists or evil intentions on the part of these scientists.\"</p><p>This stark warning from Why AI Undermines Democracy and What to Do about It, the new book by Belgian philosopher of technology and professor at the University of Vienna Mark Coeckelbergh is a well-written critique of the political effects of artificial intelligence emblematic of much of the academic literature on the dangers of big data and tech.</p><p>His argument goes something like this: AI is political because the concentration of power in the hands of unaccountable tech czars like Musk, Altman and Zuckerberg and the increasing use of new technologies by governments bent on citizen surveillance endanger our democratic system of rule of law and equality. The creation (or the use?) of these tools puts us at risk of digital totalitarianism because AI can manipulate political beliefs; algorithms, when used for governmental decision making, can deprive us of rights and liberties; AI can lead to bias and polarization; AI amplifies echo chambers and makes it harder to distinguish truth from misinformation.</p><p>While there are references to Chinese social credit scoring and digital contact tracing in South Korea (a little concerning after unironically citing Senator Josh Hawley as a source), this slim volume from Polity Press focuses more on theoretical concerns than practical solutions rooted in any specific geography or culture. It is a philosophical reflection on the potential loss of agency engendered by AI and an attempt to root such arguments in democratic theory. \"I will argue,\" he says, \"that the problem is not just voter manipulation, but that there are also other and perhaps even more important deeper ways in which AI risks eroding the very foundation of our democracies: democratic principles such as freedom, equality between citizens, fraternity, tolerance, and the rule of law, but also the basis of knowledge and trust needed for a richer, more stable, and more resilient form of democracy that can face the challenges of the twenty-first century.\"</p><p>Coeckelbergh, however, is not all doom and gloom. He usefully calls attention to the need for new democratic institutions that can provide a check and balance on the power of big tech. He concludes by calling for more democratic input rooted in participatory and deliberative values into the design of AI if we are to achieve technology that serves the common good (and arrive at a common understanding of what the common good entails). I imagine the book predates experiments like the work done by the Collective Intelligence Project and the GovLab obtaining citizen input about AI or Anthropic's constitution-drafting. He also calls for global governance of AI and treating data and tech platforms as a public commons.</p><p>While Coeckelbergh's arguments may be familiar to those well-versed in AI ethics, his book offers a concise and accessible entry point for readers new to the topic. Its strength lies in succinctly calling attention to the impact of technology on our democracy and providing an excellent review of the literature with many helpful references. Though light on novel solutions and pessimistic in its outlook, the book serves as a valuable resource for those seeking a philosophical perspective on AI's challenges to democratic principles.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-02-25",
        "views": 7680,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "108",
        "title": "New White House Guidance on AI: Strong on Skills, Short on Public Engagement",
        "picture": "https://content.thegovlab.com/assets/69669a23-0d2b-494f-86c0-2a4da2cd0506?width=800",
        "body": "<p>Imagine a government that actively listens to its citizens, quickly responds to their needs, and uses cutting-edge technology to improve their lives. This is the promise of artificial intelligence (AI) in public service.</p><p>However, the reality is that many people have had frustrating experiences trying to communicate with government agencies. Whether it's navigating a confusing website, waiting on hold for hours, or feeling like their feedback goes into a black hole, citizens often struggle to make their voices heard.</p><p>AI has the potential to change that. By automating routine tasks, analyzing large volumes of data, and facilitating more personalized interactions, as I explained in testimony before the Senate Committee on Homeland Security and Governmental Affairs, AI could help government agencies become more responsive, efficient, and user-friendly.</p><p>But as the use of AI in government expands, it also raises important questions. How can agencies ensure that AI systems are transparent, accountable, and aligned with the public interest? What steps should they take to mitigate potential risks and unintended consequences? And how can they involve citizens in the design and oversight of these powerful technologies?</p><p>The White House Office of Management and Budget (OMB) recently released final guidance to federal agencies aimed at answering these questions. The memorandum (mostly unchanged from the November draft and subsequent public comment) lays out requirements for the responsible development and use of AI in government, with a particular focus on building AI skills and expertise within the federal workforce.</p><p>While the guidance is a strong step in the right direction and usefully takes a risk-based approach to internal AI governance, it misses a key opportunity to leverage AI itself as a tool for better public engagement. As we'll see, some agencies are already pioneering innovative approaches to using AI to listen to and learn from citizens. The OMB memo could have done more to highlight and encourage these efforts and instruct agencies in how to use AI to go beyond the status quo.</p><p>Let's take a closer look at the key provisions of the OMB guidance and consider what it will take to realize the full potential of AI as a force for responsive, accountable, and human-centered government.</p><p><strong>New Roles and Responsibilities</strong></p><p>The OMB Memorandum lays out the requirements federal agencies must follow regarding their own use of AI. Within 60 days, each agency must designate a Chief AI Officer (CAIO) to coordinate AI activities, promote innovation, and manage risks. The CAIO's responsibilities include developing compliance plans, advising on resource needs, and ensuring agencies don't use AI systems that fail to meet the new standards.</p><p>Agencies also have to convene an AI Governance Board within 60 days to oversee AI use and submit biannual compliance plans to OMB. Every year, agencies must inventory their AI systems and explain how they're being used.</p><p>As Mozilla points out: \"The policy is rooted in a simple observation: not all applications of AI are equally risky or equally beneficial.\" It focuses the work of the CAIO on those high-risk uses while allowing low-risk uses to move forward unimpeded.</p><p><strong>Workforce Development</strong></p><p>One of the memo's strongest points is its emphasis on building AI skills and expertise within the federal workforce. Agencies have to assess their current AI capabilities, project future needs, and make plans to recruit, hire, train, and retain both technical and non-technical AI talent.</p><p>The memo recommends a range of strategies, including designating AI Talent Leads, offering reskilling opportunities, and adopting best practices from the Office of Personnel Management. For AI systems that could impact rights or safety, agencies must also ensure role-specific training so that human operators can effectively oversee the technology.</p><p><strong>Public Engagement</strong></p><p>The final guidance does require agencies to solicit public input on \"rights-impacting\" AI systems and use that feedback to inform decisions about whether a particular application should move forward. If the feedback suggests an AI system would do more harm than good, agencies are encouraged not to use it.</p><p>However, the memo is light on specifics when it comes to how agencies should engage the public. It lists old-fashioned mechanisms like usability testing, public hearings, and outreach to federal employee unions, but is silent on the ways AI itself could facilitate better communication between agencies and the people they serve.</p><p><strong>Missed Opportunities</strong></p><p>That's a missed opportunity. The Department of Veterans Affairs, for example, is already using AI to analyze online feedback from veterans and make it easier to understand and act on. That's leading to improved services for the men and women who have served our country.</p><p>Yet the OMB guidance does little to encourage other agencies to follow the VA's lead or explain how to do so.</p><p>Instead of seeing public engagement as integral to successful AI adoption, the memo treats it as a box to be checked. The new CAIOs are charged with promoting innovation and mitigating risk but public engagement is not elevated to one of those core responsibilities.</p><p><strong>The Path Forward</strong></p><p>Make no mistake: the OMB guidance is a step in the right direction. By emphasizing workforce development, risk management, and some public input, it lays the groundwork for the responsible use of AI in government to modernize government operations.</p><p>But realizing the full potential of AI will require a more fundamental shift. Instead of viewing public engagement as an afterthought, agencies need to recognize engagement as an invaluable tool for designing AI systems that truly serve the public interest and to make use of AI to make such engagement efficient and effective.</p><p>With a commitment to exploring how to use AI to strengthen public engagement, AI could usher in a new era of responsive, accountable, and human-centered government and use of AI aligned to the public interest. The OMB memo is a strong start, but there's still a long way to go.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-01-18",
        "views": 9900,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "109",
        "title": "New Jersey Co-Creates AI Strategy With Public-Sector Staff",
        "picture": "https://content.thegovlab.com/assets/cbc828a0-1989-4a99-88b1-f00821b326f2?width=800",
        "body": "<p>Last week the nonprofit Partnership for Public Service released the results of its annual survey assessing the satisfaction of the federal government workforce. With 2 million civilian workers, the federal government is our nation’s largest employer. Public professionals gave their agencies a 47.7 out of 100 for their willingness to use the results of the survey to make the government a better place to work. This number actually represents an improvement over last year.</p><p>These results highlight the need for a new approach to modernizing government. By directly engaging public-sector workers in AI adoption and really listening to their responses, as New Jersey is doing, we can create a more collaborative and innovative environment that not only enhances public services but also improves job satisfaction and empowers our workforce.</p><p>Artificial intelligence, particularly generative AI with its ability to create and analyze language, images and text, represents a potentially transformative set of technologies. Implementing AI can help to make the government smarter, more efficient and more responsive to the needs of residents. Just as spreadsheets revolutionized office work in the 1980s and the Internet transformed service delivery in the 2000s, AI could usher in a new era of data-driven, personalized and proactive government in the 2020s and beyond.</p><p>In New Jersey, we are already turning to AI to supply quick, high-quality responses to residents’ questions submitted around the clock to business.nj.gov. Every response from our chatbot is reviewed by a human in our Business Action Center. We are also using AI to help our call center operators get answers to questions about the Anchor property tax rebate program so that they can respond faster to residents. Just last week the federal Secretary of Labor celebrated New Jersey’s online unemployment insurance application system as a model for other states to follow. We used generative AI to rewrite and simplify forms, shaving dozens of minutes off the time it now takes to apply for this benefit. And we are using AI to provide customized advice to job seekers and those wanting to learn what training they should do to prepare for in-demand jobs. These are just a few of the ways AI could enhance public services.</p><p>However, the successful adoption of AI in the public sector is far from guaranteed. In order to use artificial intelligence responsibly and ethically, in ways that safeguard people’s privacy, minimize the risk of bias and realize the power of these tools, we should be knowledgeable about what AI can and cannot do well.</p><p>Central to responsible AI use is engaging public-sector workers — the people who will be using AI tools and are deeply knowledgeable about the programs and populations they serve — in the process of designing and adopting these technologies.</p><p>Public employees can’t just be the passive recipients of AI tools and mandates handed down from above. They need to be active participants in shaping how AI is developed, tested and used to support their missions. Everyone can use generative AI, which is driven by plain language directions. Yet in a recent report by Salesforce, 6 out of 10 IT professionals in government reported a lack of AI skills. If that represents tech-savvy staff, how much less proficiency is there among non-technical workers?</p><p>Unfortunately, organizations often overlook this critical human element when pursuing AI. In New Jersey, we are charting a different path. Gov. Phil Murphy has committed to training our public workforce in artificial intelligence, understanding its complexities and how to use it effectively and responsibly to serve the public.</p><p>To prepare for that training, New Jersey will be the first state to launch a comprehensive survey of public-sector employees’ knowledge, attitudes and interests around AI. Answers to the voluntary and anonymous 10-minute survey will inform the state’s AI upskilling plans and pilot projects.</p><p>Throughout the month of June, New Jersey will be inviting executive branch employees from about 28 agencies to share their views on AI. The survey asks about employees’ current awareness and use of generative AI tools, interest in learning more, preferred training formats, top use cases and key concerns to address. By directly engaging workers up front, using their responses to inform training and upskilling for staff, New Jersey aims to co-create an AI strategy that supports and empowers our public professionals to serve residents.</p><p>We believe that involving our workers in the design and governance of AI systems, and empowering them to experiment with AI through an innovation-friendly culture, is a model for the public and private sectors alike to follow.</p><p>Staff who work with state systems every day can help identify high-value use cases for AI, provide input on training data and model requirements, spot potential issues during testing, and give feedback to improve systems over time. When public-sector workers are engaged as co-creators, AI initiatives are more likely to deliver real value and earn buy-in.</p><p>While a survey is one of the easiest ways to reach our large workforce, interviews, focus groups, town halls, and other feedback mechanisms can all help ensure that AI is designed and deployed in ways that empower rather than alienate the public workforce and ensure we are designing technology to serve our workforce and the public interest.</p><p>The Partnership for Public Service’s survey results underscore the need for a more collaborative approach to transforming government. By putting our public servants at the center of AI adoption, as New Jersey is doing, we can create a more empowering work environment while enhancing government services. As we harness AI’s potential to revolutionize government, let us remember that success lies in the knowledge and contributions of the dedicated people who serve our communities every day.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2024-05-05",
        "views": 7860,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "110",
        "title": "Plus Ça Change: Is a Public Internet But a Dream",
        "picture": "https://content.thegovlab.com/assets/f453dddb-b00c-43aa-b81d-c086cff24499?width=800",
        "body": "<p>Over the holiday weekend, I sorted through old papers and came across this 1999 article I wrote (who knew my French used to be so good) for the French newspaper L'Humanité. I wrote about the contradiction between the promise of new media technologies as transparent, accessible tools for citizen dialogue and the reality of information overload, lack of guidance in discerning truth from falsehood, and the use of these technologies by private companies for control and surveillance.</p><p>Plus ça change.... Today, the blurring of boundaries between public and private life still leaves individuals vulnerable to pressures from both the market and the state. The concerns about the concentration of media ownership, surveillance, and the commodification of personal data have only intensified with the rise of big tech platforms and AI. Algorithms curate our information feeds, often prioritizing engagement and advertising over truth and diversity of perspectives. Echo chambers and filter bubbles have exacerbated polarization and the fragmentation of public discourse.</p><p>A contradiction is developing within democratic culture, between communication and new media. These media are presented as transparent, inexpensive, easily accessible, and capable of facilitating dialogue among all citizens. However, if everyone is supposed to be able to express themselves, it is feared that no one will listen to anyone anymore, especially since the vast amount of information disseminated - particularly on the Internet - is not accompanied by any \"instruction manual\" to help \"sort\" which information is relevant or not, true or false, etc. This problem is exacerbated by the fact that so-called \"transparent\" networks actually function - because they are tools in the hands of private companies - like techniques of control and surveillance that make our private lives \"transparent,\" which is detrimental to democracy.</p><p>Indeed, we have entered an era where the boundaries between public and private life are no longer clearly defined, which tends to place individuals in a space where they are increasingly unprotected from pressures, both from the market and the state.</p><p>More than a matter of a classic definition of \"American imperialism\" - as is often said outside the United States - it seems to me that we are dealing with a new type of relationship between the development of certain techniques in the field of media and the strategies of large global companies, which are increasingly subject to hyper-concentration, particularly in Europe today. This has the effect of shifting communication for diverse individuals from public spaces to the individual computer, whether for shopping, chatting with friends, political discussions, or simply getting information; with the risk that a gap will widen between those who have this new tool and those who, for various reasons, do not. In itself, this change is neither positive nor negative: what matters is to look at the results, particularly in terms of democratic debate and political life, while also asking how the public space can avoid being entirely subjected to market influence.</p><p>This does not seem to me to be just an American problem - even though there are public televisions in Europe and, in France, a dual system between public and private channels. In the long run, the question seems to be about the flourishing of places on the networks converging towards the creation of a new public space that can be identified as such. While I am in favor of freedom of expression, I believe that to be truly ensured, it now needs laws and regulations. But I also think that the nature of these new tools could promote the emergence of self-regulations by the citizens themselves, on a global scale.</p><p>The most important thing, if we want to move forward in this direction, is undoubtedly the development of independent \"electronic guides\" to counterbalance the power of private spaces - where advertising and information are no longer really distinguished - by facilitating access to new networks whose contents escape, or would escape, the sole logic of the market. Ultimately, one could thus conceive the existence of a \"public Internet\" that functions more like a window open to the world than a mirror where everyone is finally only reflected back to themselves and their own isolation.</p><p>To illustrate my point, I would take a single example: in 1979, 40% of Germans watched the film Shoah on television the same evening. How can we imagine a future where such a federating event in public space no longer exists? How can we not fear a future without common ground, where everything is fragmented, in time and space? Furthermore, while a certain form of individualization in the ability to choose between infinitely multiplied programs can be a factor of personal autonomy, there is also the risk of a gap between those who have this new tool and those who, for various reasons, do not.</p><p>At the same time, a reflection can be engaged on a certain type of use of these new media to improve democratic processes in real life. For example - as sometimes seen in the United States - with the establishment of public networks of citizens and experts aiming together to influence certain decisions. This is not without risks, but I also believe that the nature of these new technological tools could promote the emergence of self-regulations by the citizens themselves, on a global scale.</p><p>Just as the prospect of installing electronic voting everywhere in order to remedy the crisis of representation seems fraught with danger even if it can be useful, democracy cannot be synonymous with speed: it requires, on the contrary, time for reflection, debate, and choice; in short, a certain slowness.</p><p>But it also requires engagement in the process of building new institutions, using new technologies if necessary, to make the participation of individuals in all decisions that concern them more effective.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2023-08-10",
        "views": 5560,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "111",
        "title": "New Jersey is turning to AI to improve the job search process",
        "picture": "https://content.thegovlab.com/assets/d531d91c-4f53-4d10-866a-1026046f40bc?width=800",
        "body": "<p>While people are flocking to new roles like prompt engineer and AI ethicist, the technology is also predicted to put many jobs at risk, including computer programmers, data scientists, graphic designers, writers, lawyers.</p><p>Little wonder, then, that a national survey by the Heldrich Center for Workforce Development found an overwhelming majority of Americans (66%) believe that they “will need more technological skills to achieve their career goals.” One thing is certain: Workers will need to train for change. And in a world of misinformation-filled social media platforms, it is increasingly important for trusted public institutions to provide reliable, data-driven resources.</p><p>In New Jersey, we’ve tried doing just that by collaborating with workers, including many with disabilities, to design technology that will support better decision-making around training and career change. Investing in similar public AI-powered tools could help support better consumer choice across various domains. When a public entity designs, controls and implements AI, there is a far greater likelihood that this powerful technology will be used for good.</p><br><h4>USING AI TO PREPARE FOR THE AI FUTURE</h4><br><p>In New Jersey, the public can find reliable, independent, unbiased information about training and upskilling on the state’s new MyCareer website, which uses AI to make personalized recommendations about your career prospects, and the training you will need to be ready for a high-growth, in-demand job.</p><p>The New Jersey Office of Innovation (full disclosure: I founded the Office and now serve there as an unpaid advisor) and the State’s Department of Labor quietly launched MyCareer. last month. MyCareer is actually two websites in one: Career Navigator and Training Explorer.</p><br><h4>TRAINING EXPLORER: DATA-DRIVEN COURSE COMPARISONS</h4><br><p>On Training Explorer, anyone can search for training opportunities. Type in “plumbing” to search for online and in-person training programs. The site returns 72 options with details for each about location, language, cost, wheelchair accessibility and time-to-complete. But, more importantly, it draws on real-time labor market data to indicate that plumbing is an in-demand career, which has openings. Sinks still get clogged in an AI world. </p><p>For each course, Training Explorer tells you the average income and employment rate of people who took the course. Today, a $17K plumbing course from a private college in New Jersey has a 17% employment rate; yet a $1,299 course from a publicly funded institution has a 96.1% post-course employment rate. Moreover, the after-program salary for those who went to the first school was $27,556 while graduates from the public program went on to earn $43,256. Training providers furnish the state with data about who took their programs, which the state then matches privately and securely to wage records to deliver this anonymized outcome data. All employers subject to unemployment compensation laws have to submit wage data to the state where they operate. With such outcomes data, residents can inform their decision-making and training providers can improve their offerings.</p><br><h4>CAREER NAVIGATOR: PERSONALIZED JOB RECOMMENDATIONS</h4><br><p>The other half of this twin website is the Career Navigator. Logging in with my gmail or LinkedIn like any commercial website, the site prompts me to upload my résumé so I don’t have to type in my education or work history. Based on the resumé information, which the state stores securely in its protected cloud, Career Navigator offers personalized career advice about those in-demand careers in New Jersey that are well-matched to my skills. It shows me predicted pay ranges and job availability along with direct links to job opportunities. To give me individualized advice on current job openings, it is drawing on real-time data from the National Labor Exchange. The Exchange is a public-private data partnership among 300,000 employers and state workforce agencies, designed to connect workers with job openings.</p><p>While we are accustomed to Netflix recommending which movie to watch next or Amazon suggesting a book to buy, government websites are usually one-size-fits all. Taking advantage of the data the government collects from business and individuals, however, artificial intelligence can help government agencies provide information personalized to each individual.</p><p>After ingesting the data from my resume, Career Navigator uses double-debiased machine learning (AI designed to remove biases) to predict other occupations with which I might have the most skills in common in order to help me identify new career opportunities. Many careers have skills in common. However, the creators explain to me, even if being a surgeon demands many of the same skills as another job, the system will not tell just anyone to become a surgeon. It will only propose jobs that require similar levels of education attainment rather than telling someone with a high school diploma about a job that requires medical school.</p><p>In addition to using AI to match you to a new career, the site also makes personalized recommendations about training, suggesting those learning opportunities that are most likely to lead to a wage bump. Those recommendations are driven by the data on whether people with similar skills to mine have taken the course and seen their pay increase. Finally, it can make recommendations based on how well it can build upon someone’s existing skill set, how much more money they can make, and whether there will be enough job openings for them to find a job.</p><p>While the platform is revolutionary, there are still some challenges to address. For instance, not all training providers consistently report comprehensive data, leading to gaps in the information available to users. Improving data collection and reporting processes will be crucial for enhancing the platform’s effectiveness. The site struggles to explain in plain English to job seekers what it means to have machine learning help with evidence-based recommendations. Future iterations on this site will need to address these issues. But giving job seekers powerful search tools rooted in reliable data that can help them make informed decisions is game changing.</p><br><h4>THE IMPORTANCE OF PUBLIC AI-POWERED RESOURCES<br></h4><br><p>By providing job seekers with personalized, data-driven recommendations and transparent information about training outcomes, the MyCareer platform empowers individuals to make informed decisions about their careers in the face of technological change. As a state with millions of workers to support and protect, it’s hard to give each one the tailored services that might best lead them to success. Such AI-enabled tools make it possible to scale up personalized services to help New Jerseyans climb career ladders or switch to a more rewarding profession. As AI continues to reshape the workforce, initiatives like these will be essential for helping workers navigate the challenges and opportunities ahead from AI’s impact on the future of work.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2023-12-08",
        "views": 2200,
        "tags": ["#Democracy","#AI","#Generative AI"]
      },
      {
        "id": "112",
        "title": "Reviving Our Inner Child: 'Co-Intelligence' and the Promise of AI as a Creative Catalyst",
        "picture": "https://content.thegovlab.com/assets/6226693f-f84f-4293-bbd2-c431f6ebfcab?width=800",
        "body": "<p>In his new book on AI Co-Intelligence: Living and Working with AI, Wharton professor Ethan Mollick modestly declares that “while we didn’t think of ourselves as having a special skill in prompting, we found that we were very good at making AI dance to our tune.” Mollick may be right. While he was early to experiment with tools like ChatGPT and then to share his experiences via academic papers and his substack blog One Useful Thing before most people had tried ChatGPT, today anyone can get smarter and more productive using generative AI.</p><p>That’s the point of this slim but engaging volume. Because generative AI is trained on billions of words from the Internet and has mastered the patterns and structures of the English (and other) languages, we can all use these tools as a thinking companion for a variety of tasks. We should “try inviting AI to help you in everything you do,” he argues. This “co-intelligence” can “improve our own decision-making, helping us to reflect on our own choices” and the more we use it, the faster we will discover all the intriguing, ingenious, and exciting uses for these digital companions.</p><p>In contrast to older tools like the lever or the loom, this is the first set of tools that augment how we think, rather than how we move in the world. The concept of 'co-intelligence' represents a significant departure from the way we typically approach human-machine interactions.</p><p>In traditional software and automation, we interact with our tools in a highly structured and predictable manner. When we use a calculator or a spreadsheet, for example, we input specific commands or formulas, and the machine generates outputs based on a predetermined set of rules. We know exactly what to expect from the machine, and there is little room for surprise or creativity. In contrast, the AI tools Mollick describes in Co-Intelligence exhibit a level of unpredictability that more closely resembles human interaction. When we engage with a language model like ChatGPT, we don't always know what kind of response we'll receive. This element of surprise is what makes the interaction feel more like a conversation with a knowledgeable and creative partner than a simple input-output transaction.</p><p>If you are a reader of Mollick’s excellent and timely blog, you won’t find anything new in here. The book is a highlights reel from One Useful Thing--and that’s a compliment--boiled down to a 40K portable primer on large language models, what they are, and how they are likely to impact education, work and society written in a lively, first-person account. That impact will be momentous but the shape it will take and how fast we will arrive at that future are both uncertain. The book offers a greatest hits of the issues: hallucination, bias, the death of homework. Mollick’s anodyne musings about the inconclusive impact of AI on jobs can also be found in the latest consultant reports, think tank white papers, other research, or, even better, in his own Harvard Business Review article on how AI improves worker productivity.</p><p>Where Co-Intelligence bristles with energy is when Mollick shares his own sense of wonder at inhabiting the AUT or astonishing universe of AI tech. Mollick repeatedly cuts and pastes into the manuscript both his prompts and ChatGPT’s replies. It’s a not insignificant chunk of the book, which was unabashedly written with such prompts as “Make this better, in the style of a bestselling popular book about AI.”</p><p>It worked.</p><p>The reader is a bystander to Mollick’s conversations (or is it an inner monologue?) with the team of AI personas he created to help him write the book. From the pompous, critical editor to the eager collaborator to the ordinary, albeit confused reader, Mollick has created a cast of characters who work alongside him.</p><p>Some readers might find the frequent excerpts from ChatGPT to be annoying, especially given the small volume’s $30 price tag. But I loved reading about Mollick talking to his imaginary buddies in the way that a parent enjoys seeing their child blossom as he talks to his magical but invisible companions. The book reminds us that as immigrants to this new AI frontier, we will get the most from these tools when we stop thinking of them as the better calculators and word processors they are and, instead, treat them like the reincarnation of our childhood friends with whom we think aloud about our creative endeavors. Today, for example, I asked Claude a question and it anthropomorphically responded: “One thought that occurred to me as I was reading….”</p><p>When we were kids, we used our conversations with our imaginary playmates to talk through our experiences. As we grew older, they faded away as did our imaginations. As AI tools become more advanced and attuned to our needs, they may become an increasingly integral part of our lives, not just as tools but as collaborators and partners in our personal and professional growth. (check out this example of LinkedIn founder Reid Hoffman talking to his AI-self.)</p><p>The challenge, then, is to embrace the potential of AI to bring out our imaginations through more conversation, allowing AI to reinvigorate our creativity and productivity. But there is also a risk. AI can make us lazy and careless when we depend on it too much. We might lose ourselves and our agency to the tools.</p><p>As we step into this new era of co-intelligence, we have the opportunity to reconnect with a part of ourselves that many of us have lost touch with: our childhood imagination. Mollick's book serves as a valuable companion, reminding us how not, as Henry David Thoreau said, to become “the tools of our tools” but, instead, how to use them to rediscover our own creative potential. In the end, the book does not have much space to offer novel prescriptions about societal improvement. For Mollick, the true power of co-intelligence lies in our willingness to embrace it as a catalyst for personal improvement and individual creativity.</p>",
        "authorName": "Beth Simone Noveck",
        "date": "2023-09-30",
        "views": 3300,
        "tags": ["#Democracy","#AI","#Generative AI"]
      }
    ]
  }
}
